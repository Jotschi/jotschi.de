<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gfx on Jotschi&#39;s Blog</title>
    <link>http://jotschi.de/categories/gfx/</link>
    <description>Recent content in Gfx on Jotschi&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Aug 2009 23:20:56 +0000</lastBuildDate>
    <atom:link href="http://jotschi.de/categories/gfx/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>GLSL shader to handle multiple projections onto the same surface</title>
      <link>http://jotschi.de/2009/08/17/glsl-shader-to-handle-multiple-projections-onto-the-same-surface/</link>
      <pubDate>Mon, 17 Aug 2009 23:20:56 +0000</pubDate>
      
      <guid>http://jotschi.de/2009/08/17/glsl-shader-to-handle-multiple-projections-onto-the-same-surface/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;For my project &amp;lt;a href=&#34;http://www.jotschi.de/?page_id=320&#34;&amp;gt;Generating City Models By Using Panorama Images&amp;lt;/a&amp;gt; i have to write a fragment shader that handles multiple projections onto the same surface. The shader must contain condition in which the projection should be handled. In my case i had to decide which projection source should be used to texture the desired model. &amp;lt;a id=&#34;more&#34;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;a id=&#34;more-363&#34;&amp;gt;&amp;lt;/a&amp;gt;
I didn&amp;#8217;t want to do alpha blending of each texture. I wanted that only the projection source will be used that would produce the best projection image. For a good projection the angle of which the projection &#39;light&#39; falls onto the surface must be very low. The second parameter is the distance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;For example, imagine a scene with 20 projection sources. One source is very near the target surface but the angle in which the &#39;light&#39; falls onto the surface is very big so the texture produced by such a source wouldn&amp;#8217;t be that good. Instead there is another projection source which&amp;#8217;s projections angle is very low but the distance is larger than that of the near source. My shader will now pick the second projection source because the first does not match its condtions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Draft of my target shader:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;uniform int nTextures;
uniform sampler2DArray textures;
uniform vec3[] projectPos;
varying vec3 glPos;
varying vec3 normalVec;
void main() {
  int currentTexture=0;
  float minAngle = 360.0f;
  float angleThreshold = 10.0f;
 // Select the texture with the lowest angle
  for(int i =0; i &amp;lt;nTextures; i++){
   float alpha = texture2DArray(textures,vec3(gl_TexCoord[0].xy, i)).a;
   if(alpha != 1.0f) {
     //TODO check calculation of projDirection
      vec3 projDirection =(projectPos-glPos);
      float angle = dot(normalVec,projDirection*-1);
      if(angle&amp;lt;minAngle) {
          minAngle = angle;
      }
    }
  }
  float minDistance= -1.0f;
  // Select the texture with the lowest distance among those around the lowest angle
  for(i=0; i &amp;lt;nTextures; i++){
   float alpha = texture2DArray(textures,vec3(gl_TexCoord[0].xy, i)).a;
   // Condition 1: Must be visible
   if (alpha != 1.0f) {
     //TODO check calculation of projDirection
     vec3 projDirection =( projectPos-glPos);
     float angle = dot(normalVec,projDirection);
     // Condition 2: The angle must be between +-10.0f of the lowest angle
     if(angle&amp;lt;(minAngle+angleThreshold) || angle&amp;gt;(minAngle+angleThreshold)) {
     // TODO check calculation of distance between both points within RÂ³
    float dx = projectPos[i].x - glPos.x;
  float dy = projectPos[i].y - glPos.y;
  float dz = projectPos[i].z - glPos.z;
  float d = Math.sqrt(dx * dx + dy * dy + dz * dz);
    // select the texture with the lowest distance
    if(d&amp;lt;minDistance || minDistance  ==-1.0f)  {
          minDistance = d;
          currentTexture = i;
    }
        }
   }
  }
gl_FragColor = texture2DArray(textures,vec3(gl_TexCoord[0].xy, currentTexture));
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Maybe there is a better way to check if the given sample contains a texture value at the given texture coordinate than checking its alpha value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I use a uniform to handle the size of the sampler2DArray textures array. Maybe you can extract that information within the shader&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I use uniform vec3[] projectPos; to handle over the positions of the projection sources. Maybe this can be stored in the build in gl_LightSource array?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OpenPhotoVR</title>
      <link>http://jotschi.de/2009/06/11/openphotovr/</link>
      <pubDate>Thu, 11 Jun 2009 10:46:27 +0000</pubDate>
      
      <guid>http://jotschi.de/2009/06/11/openphotovr/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;While searching for a solution to get mono and silverlight and the whole M$ stuff working under my debian installation i just found &amp;lt;a href=&#34;http://openphotovr.org/#a90&#34;&amp;gt;OpenPhotoVR&amp;lt;/a&amp;gt;. OpenPhotoVR is a pseudo-3D photo browser using Flash instead of silverlight. However the matching of the images is not done automatically.&amp;lt;a id=&#34;more&#34;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;a id=&#34;more-294&#34;&amp;gt;&amp;lt;/a&amp;gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&amp;lt;object width=&#34;400&#34; height=&#34;300&#34;&amp;gt;
&amp;lt;param name=&#34;movie&#34; value=&#34;http://openphotovr.org/main.swf&#34;&amp;gt;&amp;lt;/param&amp;gt;
&amp;lt;param name=&#34;FlashVars&#34; value=&#34;id=a299&#34;&amp;gt;&amp;lt;/param&amp;gt;
&amp;lt;embed src=&#34;http://openphotovr.org/main.swf&#34;   FlashVars=&#34;id=a299&#34;   type=&#34;application/x-shockwave-flash&#34;     width=&#34;400&#34; height=&#34;300&#34; &amp;gt;&amp;lt;/embed&amp;gt;
&amp;lt;/object&amp;gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;{% youtube 0f5NuHzGXJA %}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Since the &amp;lt;a href=&#34;http://phototour.cs.washington.edu/bundler/&#34;&amp;gt;bundler code&amp;lt;/a&amp;gt; which is used to match images in images is free it should be possible to create an automatically way of matching the images.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automated, Scalable, Airborne Only, 3D Modeling </title>
      <link>http://jotschi.de/2009/04/23/automated-scalable-airborne-only-3d-modeling/</link>
      <pubDate>Thu, 23 Apr 2009 22:58:44 +0000</pubDate>
      
      <guid>http://jotschi.de/2009/04/23/automated-scalable-airborne-only-3d-modeling/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I just found this page which describes methods of 3D City Modeling. They use airborn and ground based lasers to get the geometric information that is needed to complete the model. Keep in mind that Google StreetView Cars are also equiped with lidar.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www-video.eecs.berkeley.edu/~avz/aironly.htm&#34; class=&#34;bare&#34;&gt;http://www-video.eecs.berkeley.edu/~avz/aironly.htm&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Also take a look at this interesting techtalk:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://video.google.com/videoplay?docid=1783762882645705066&amp;amp;ei=P9XwSe2fJJ7e2gL3kJWxDA&amp;amp;q=Avideh+Zakhor&amp;amp;hl=en&amp;amp;client=iceweasel-a&#34; class=&#34;bare&#34;&gt;http://video.google.com/videoplay?docid=1783762882645705066&amp;amp;ei=P9XwSe2fJJ7e2gL3kJWxDA&amp;amp;q=Avideh+Zakhor&amp;amp;hl=en&amp;amp;client=iceweasel-a&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Removal of Humans In Motion Pictures</title>
      <link>http://jotschi.de/2009/01/04/removal-of-humans-in-motion-pictures/</link>
      <pubDate>Sun, 04 Jan 2009 00:16:18 +0000</pubDate>
      
      <guid>http://jotschi.de/2009/01/04/removal-of-humans-in-motion-pictures/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What do we need to remove a person from a video source?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;At first we have to identify the person in the video. This could be done by using face tracking.
{% youtube 5sPh6-J9wOA %}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Once the location of a face of a moving person is identified within the video source we can outline the persons body. The outline of the person can be estimated by analyzing the optical flow of the video source. The corresponding pixels of the head that was pinpointed by using face tracking will most likely move with the same direction and speed as the pixels from other body parts. The background of the person can be used as a static plane by analyzing the camera movements. Once the background has been defined an estimation of the persons outline is more reliable because in the most cases the motion and direction of the background pixels and the motion and direction of the persons pixels vary.
Moreover it is also possible to use depth estimation of the image to extrapolate different layers of the image that could be assigned to the person or the background to support the identification of the persons pixels.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Once the person has been identified the person can be removed and the gap can be filled by using texture synthesis.
{% youtube fWwy2gZuD6E %}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Harry Shum - Microsoft Research Asia&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In fact is is also possible to fill the gap of the person by using pixel areas from subsequent frames were no person was covering the same area. This approach is illustrated here:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.vision.huji.ac.il/demos/removal/removal.html&#34; class=&#34;bare&#34;&gt;http://www.vision.huji.ac.il/demos/removal/removal.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>